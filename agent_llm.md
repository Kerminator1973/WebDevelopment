# Агентская модель в LLM

MCP = _Model Context Protocol_.

Разработчик MCP - Anthropic, в 2024 году.

RAG (Retrieval-Augmented Generation) в LLM — это техника, которая улучшает ответы больших языковых моделей, позволяя им получать и использовать внешние, актуальные данные (из баз знаний, документов, интернета) перед генерацией ответа, а не полагаться только на свои "замороженные" обучающие данные. Вместо того чтобы полагаться только на знания, запомненные в параметрах модели, система динамически подбирает релевантные фрагменты из внешних источников (баз знаний, документов, веб‑страниц) и использует их как контекст для генерации ответа.

Как работает процесс:

- Пользователь вводит вопрос. Специальный encoder преобразует запрос в вектор
- Вектор сравнивается с векторами документов. Возвращаются топ‑k наиболее релевантных фрагментов
- Найденные фрагменты конкатенируются с запросом. При необходимости добавляются подсказки (prompt engineering)
- LLM генерирует текст, опираясь на предоставленный контекст. Поскольку модель «видит» актуальные данные, она может давать более точные и свежие ответы, чем при чистой генерации
- Маленькая модель‑классификатор проверяет, соответствует ли ответ найденному контексту, и при необходимости запрашивает дополнительные фрагменты

Пример подсказки:

```
Вопрос: {query}
Контекст: {retrieved_texts}
Ответь, используя только информацию из контекста.
```

MCP - это как USB-C для агентских систем.

LLM - это клиент агентских систем.

Примеры агентов: PostgreSQL, Google Drive, GitHub.

Чаще всего агенты предоставляют LLM доступ к ресурсам: папкам и файлам, что позволяет LLM сформировать больший контекст, получая доступ к файлам проекта, настройкам, и т.д.

Агенты встраиваются в IDE: [Windsurf](https://windsurf.com/) (см. Cascade), [Cursor](https://cursor.com/) (см. Composer), [ZED](https://zed.dev/) разработанный на Rust, [Google Antigravity](https://antigravity.google/), [Trae IDE](https://www.trae.ai/), Visual Studio Code, Visual Studio 2026, GigaIDE, KodaCode, IntelliJ AI, Amazon CodeWhisperer Studio, GitHub Codespaces. В большинстве случаев, в качестве основы IDE используется Visual Studio Code.

Российский агент встраиваемый в VSCode - [KodaCode](https://kodacode.ru/). Работает с зарубежными LLM без прокси. Поддерживает работу бесплатный режим с китайскими LLM (MiniMax M2.1, Qwen3, Kimi K2 Thinking, GLM 4.6). Требует автризации на GitHub. На начало 2026 года бесплатный, но планируется переход на подписочную модель с поддержкой нескольких бесплатных LLM. В начале 2026 поддержка IDE JetBrains. Разработчики - бывшие сотрудники СБЕРа, разрабатывавшие GigaCode.

>Cursor - в режиме **Plan** задаёт уточняющие вопросы и формирует Markdown-файл с описанием задачи и списком того, чтоб нужно сделать с кодом. Далее Cursor работает уже с этим списком.
>
>В бесплатной версии есть GPT 5.2. Хорошо подходит для проектирования, прототипов и быстрых экспериментов. Позволяет выполнять регистрацию на "10 минутные" почтовые аккаунты.
>
>Примеры 10-минутной (disposable) почты: [TMailor.com](https://tmailor.com/), [Moakt](https://moakt.com/ru), [MailSAC](https://mailsac.com/).
>
>Рекомендуется для прочтения [Осознанный вайб-кодинг](https://habr.com/ru/articles/982452/) by davidaganov. Интересен подход создания первоначального контекста для генерации приложения:
>
>- Появляется идея. Например, сервис для учёта финансов. Я обсуждаю её с ChatGPT: в каком виде лучше реализовать, на каких технологиях, как это можно хостить, что в итоге хочется получить. Прошу задать 10–15 вопросов, ответы на которые помогут при построении плана.
>-После этого прошу построить план с учётом моих ответов и контекста диалога. Получаю MD файл, внимательно его просматриваю, собираю свои вопросы, что-то уточняю и правлю. Когда файл становится понятным и логичным - считаю его готовым к работе.
>-Кладу этот файл в пустую папку проекта, передаю его как контекст в чат и прошу построить архитектуру и установить зависимости. Параллельно вручную накидываю свои конфиги для eslint, prettier, editorconfig и правлю мелочи, которые для меня важны и привычны.

Рекомендуется для прочтения статья [Claude Code научили работать с Chrome. Вот насколько это опасно](https://habr.com/ru/news/978712/) by **python_leader**. Появилась новый вид уязвимостей **Prompt Injection** - документы, которые включаются в контекст LLM, например, электронные письма содержат вставки, вынуждающие LLM формировать команды, наносящие ущерб пользователю системы, например, удалять всю почту пользователя без подтверждения. Браузерные расширения Anthropic развиваются в направлении создания специальной политики информационной безопасности, которая требует подтверждения пользователя для критичных операций выполняемых агентами, таких как: публикации, покупки, предоставление другим пользователям доступа к ресурсам системы.

Agent Mode в Visual Studio Code: `File -> Preference -> Settings`, искать настройку `chat.agent.enabled`.

Agent Mode в Visual Studio Code получает доступ к окну "Output" (результаты компиляции), Linter-у, умеет напрямую работать с браузером (доступ к DOM, возможно - отладка). Также имеет доступ к консоли (Terminal), т.е. LLM может выполнять команды операционной системы.

Можно добавить дополнительные MCP-сервера через магазин plug-in-ов.

Почему доступ для разработчиков в сильных моделях стоит дорого: LLM активно взаимодействует с агентами через MCP, в частности, для устранения ошибок сборки и улучшения кода на основании Linter-ов. Такого рода взаимодействие требует огромного количества промптов.

Выгрузка больших проектов, т.е. создание большого контекста, также приводит к активному расходу токенов.

Резюмируя причины дороговизны мощного ИИ:

- требуется более мощное оборудование (размер ОЗУ и SSD) для создания инференса. Мощность LLM определяется сотниями миллиардов параметров
- загрузка контекста в инференс из локальной машины требует большего объёма памяти
- частота выполнения запросов к LLM от агентов значительно выше, чем в классической модели

## Сводная таблица LLM

На первый взгляд, критичным для сравнения характеристиками являются количество параметров языковой модели и размер окна контекста.

Параметры – это численные веса, которые модель обучает на больших корпусах текста. Каждый параметр — скаляр (обычно 32‑ или 16‑битное число), определяющий силу связи между нейронами в сети. Большее количество параметров обычно позволяет захватить более тонкие закономерности языка, улучшая точность, генерацию и способность решать сложные задачи.

Принято сравнивать классы LLM, исходя из количества параметров:

- 7–9B - часто такие LLM размещаются локально
- 14–32B - часто размещаются локально, но требуют дорогостоящего оборудования
- 70B+ - работают в облачной инфраструктуре на специализированном оборудовании

Размер окна контекста – это максимальное количество токенов, которое модель может принимать в одном запросе (входном фрагменте) и учитывать при генерации ответа. Токен — элемент текста, обычно слово, часть слова или символ; в большинстве моделей один токен ≈ 4 символа английского текста. При получении входа модель строит последовательность токенов и применяет механизм _attention_ к каждому из них. Окно ограничивает длину этой последовательности; если вход превышает лимит, старые токены отбрасываются (обычно с начала).

Также на качество работы LLM влияют архитектурные улучшения (например, более эффективные _attention_‑механизмы), объём и разнообразие обучающих данных, пост-тюнинг, а также техника дообучения. Всё это может дать значительный прирост качества даже при том же числе параметров.

Контекст: у многих моделей “максимум” не всегда равен “полезному” контексту (качество внимания падает на очень длинных диалогах в зависимости от модели).

Для LLM специализирующихся на задачах программирования ключевые метрики следующие:

- Качество редактирования (diff‑мышление): умение минимально править код без поломок
- Надёжность многошаговых изменений: поддержание инвариантов, типов, тестов
- Работа с репозиторием: способность удерживать структуру проекта (даже при RAG/подгрузке файлов)
- Дебаг по логам: причинно‑следственные гипотезы, аккуратные проверки
- Соблюдение формата: JSON, API‑контракты, схемы, строгие инструкции

Сравнительная таблицы открытых и коммерческих LLM:

| Модель | Параметры (≈) | Размер окна контекста |
|--------|---------------|-----------------------|
| **LLaMA 2 7B** | 7 B | 4 KB (≈ 4096 токенов) |
| **LLaMA 2 13B** | 13 B | 4 KB (≈ 4096 токенов) |
| **Mistral 7B‑Instruct** | 7 B | 8 KB (≈ 8192 токенов) |
| **Mixtral‑8×7B‑Instruct** | 46 B (8 × 7 B) | 16 KB (≈ 16384 токенов) |
| **Gemma 2 9B** | 9 B | 8 KB (≈ 8192 токенов) |
| **Phi‑3 mini‑4K** | 3.8 B | 4 KB (≈ 4096 токенов) |
| **Phi‑3 medium‑128K** | 14 B | 128 KB (≈ 131 072 токенов) |
| **Qwen‑1.5‑7B** | 7 B | 8 KB (≈ 8192 токенов) |
| **Qwen‑1.5‑14B** | 14 B | 8 KB (≈ 8192 токенов) |
| **OpenChat‑3.5‑7B** | 7 B | 4 KB (≈ 4096 токенов) |

*Параметры указаны в миллиардах (B). Размер окна контекста – максимальное количество токенов, которое модель может принимать за один запрос.*

| Модель | Параметры (≈) | Размер окна контекста |
|--------|---------------|-----------------------|
| **GPT‑4‑Turbo** (OpenAI) | 100 B + (точные цифры не раскрыты) | 128 KB (≈ 131 072 токенов) |
| **Claude 3 Opus** (Anthropic) | 200 B + (не раскрыто) | 100 KB (≈ 100 000 токенов) |
| **LLaMA 3 70B** | 70 B | 8 KB (≈ 8192 токенов) |
| **Mistral‑Large** | 123 B | 16 KB (≈ 16384 токенов) |
| **Mixtral‑8×22B** | 176 B (8 × 22 B) | 32 KB (≈ 32768 токенов) |
| **Gemma 2 27B** | 27 B | 16 KB (≈ 16384 токенов) |
| **Phi‑3 medium‑128K** | 14 B | 128 KB (≈ 131 072 токенов) |
| **Qwen‑2‑72B** | 72 B | 32 KB (≈ 32768 токенов) |
| **Gemini 1.5‑Pro** (Google) | 140 B + (не раскрыто) | 100 KB (≈ 100 000 токенов) |
| **DeepSeek‑Coder 33B** | 33 B | 8 KB (≈ 8192 токенов) |

Точная информация по современным LLM обычно не раскрывается. Так, например, не удалось найти данных по количеству параметров Kimi K2. Предположительно, что Gemini 3 Pro, GPT 5.2, Cloude 4.5 кратно мощнее LLM предыдущих поколений.

Обобщая:

- Чем больше параметров, тем больше информации закодировано в LLM
- Размер контекста позволяет LLM работать не с несколькими отдельными фразами запроса, а с кодом проекта, или сложными промптами, описывающими разрабатываемую систему комплексно
- Современные LLM знают актуальные стандарты языков программирования и библиотеки
- Пост-тьюнинг позволяет представлять результаты работы в форматах, применимых для автоматизации
- Надёжность многошаговых изменений позволяет улучшать результат работы LLM итеративно, по этапно
