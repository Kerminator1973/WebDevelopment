# Агентская модель в LLM

MCP = _Model Context Protocol_.

Разработчик MCP - Anthropic, в 2024 году.

RAG (Retrieval-Augmented Generation) в LLM — это техника, которая улучшает ответы больших языковых моделей, позволяя им получать и использовать внешние, актуальные данные (из баз знаний, документов, интернета) перед генерацией ответа, а не полагаться только на свои "замороженные" обучающие данные. Вместо того чтобы полагаться только на знания, запомненные в параметрах модели, система динамически подбирает релевантные фрагменты из внешних источников (баз знаний, документов, веб‑страниц) и использует их как контекст для генерации ответа.

Как работает процесс:

- Пользователь вводит вопрос. Специальный encoder преобразует запрос в вектор
- Вектор сравнивается с векторами документов. Возвращаются топ‑k наиболее релевантных фрагментов
- Найденные фрагменты конкатенируются с запросом. При необходимости добавляются подсказки (prompt engineering)
- LLM генерирует текст, опираясь на предоставленный контекст. Поскольку модель «видит» актуальные данные, она может давать более точные и свежие ответы, чем при чистой генерации
- Маленькая модель‑классификатор проверяет, соответствует ли ответ найденному контексту, и при необходимости запрашивает дополнительные фрагменты

Пример подсказки:

```
Вопрос: {query}
Контекст: {retrieved_texts}
Ответь, используя только информацию из контекста.
```

Рекомендуется для прочтения статья [RAG-системы: что это такое, принципы работы, архитектура и ограничения](https://habr.com/ru/articles/989000/) by Mister_Zero.

RAG позволяет, в том числе, бороться с галлюцинациями ИИ. Обычно галлюцинации возникают по двум фундаментальным причинам:

- у модели нет необходимой информации, чтобы корректно ответить на вопрос
- модель оптимизирована под генерацию правдоподобного ответа, а не под отказ от ответа

RAG (Retrieval-Augmented Generation) — это подход, при котором большая языковая модель перед генерацией ответа получает дополнительный контекст из внешнего источника знаний: базы данных, документов, логов, API и т.д.

RAG не делает модель умнее. Он делает её более информированной и, как следствие, более надёжной.

MCP - это как USB-C для агентских систем.

LLM - это клиент агентских систем.

Примеры агентов: PostgreSQL, Google Drive, GitHub.

Чаще всего агенты предоставляют LLM доступ к ресурсам: папкам и файлам, что позволяет LLM сформировать больший контекст, получая доступ к файлам проекта, настройкам, и т.д.

Агенты встраиваются в IDE: [Windsurf](https://windsurf.com/) (см. Cascade), [Cursor](https://cursor.com/) (см. Composer), [ZED](https://zed.dev/) разработанный на Rust, [Google Antigravity](https://antigravity.google/), [Trae IDE](https://www.trae.ai/), Visual Studio Code, Visual Studio 2026, GigaIDE, KodaCode, IntelliJ AI, Amazon CodeWhisperer Studio, GitHub Codespaces. В большинстве случаев, в качестве основы IDE используется Visual Studio Code.

Российский агент встраиваемый в VSCode - [KodaCode](https://kodacode.ru/). Работает с зарубежными LLM без прокси. Поддерживает работу бесплатный режим с китайскими LLM (MiniMax M2.1, Qwen3, Kimi K2 Thinking, GLM 4.6). Требует авторизации на GitHub. На начало 2026 года бесплатный, но планируется переход на подписочную модель с поддержкой нескольких бесплатных LLM. В начале 2026 поддержка IDE JetBrains. Разработчики - бывшие сотрудники СБЕРа, разрабатывавшие GigaCode. Kimi Code предлагает несколько тарифных планов, самый дешевый из которых стоит $19/месяц.

>Cursor - в режиме **Plan** задаёт уточняющие вопросы и формирует Markdown-файл с описанием задачи и списком того, чтоб нужно сделать с кодом. Далее Cursor работает уже с этим списком.
>
>В бесплатной версии есть GPT 5.2. Хорошо подходит для проектирования, прототипов и быстрых экспериментов. Позволяет выполнять регистрацию на "10 минутные" почтовые аккаунты.
>
>Примеры 10-минутной (disposable) почты: [TMailor.com](https://tmailor.com/), [Moakt](https://moakt.com/ru), [MailSAC](https://mailsac.com/).
>
>Рекомендуется для прочтения [Осознанный вайб-кодинг](https://habr.com/ru/articles/982452/) by davidaganov. Интересен подход создания первоначального контекста для генерации приложения:
>
>- Появляется идея. Например, сервис для учёта финансов. Я обсуждаю её с ChatGPT: в каком виде лучше реализовать, на каких технологиях, как это можно хостить, что в итоге хочется получить. Прошу задать 10–15 вопросов, ответы на которые помогут при построении плана.
>-После этого прошу построить план с учётом моих ответов и контекста диалога. Получаю MD файл, внимательно его просматриваю, собираю свои вопросы, что-то уточняю и правлю. Когда файл становится понятным и логичным - считаю его готовым к работе.
>-Кладу этот файл в пустую папку проекта, передаю его как контекст в чат и прошу построить архитектуру и установить зависимости. Параллельно вручную накидываю свои конфиги для eslint, prettier, editorconfig и правлю мелочи, которые для меня важны и привычны.

>Рекомендуется к прочтению [ИИ — это не пузырь](https://dtf.ru/id3314715/4752872-ii-i-mashinnoe-obuchenie-razvitie-i-perspektivy) by Ostov Larion. Короткая статья с тезисами о том, как работает сознание и почему его можно имитировать с помощью ИИ. Ссылка на "Теорию Задачного Пространства".

Рекомендуется для прочтения статья [Claude Code научили работать с Chrome. Вот насколько это опасно](https://habr.com/ru/news/978712/) by **python_leader**. Появилась новый вид уязвимостей **Prompt Injection** - документы, которые включаются в контекст LLM, например, электронные письма содержат вставки, вынуждающие LLM формировать команды, наносящие ущерб пользователю системы, например, удалять всю почту пользователя без подтверждения. Браузерные расширения Anthropic развиваются в направлении создания специальной политики информационной безопасности, которая требует подтверждения пользователя для критичных операций выполняемых агентами, таких как: публикации, покупки, предоставление другим пользователям доступа к ресурсам системы.

Agent Mode в Visual Studio Code: `File -> Preference -> Settings`, искать настройку `chat.agent.enabled`.

Agent Mode в Visual Studio Code получает доступ к окну "Output" (результаты компиляции), Linter-у, умеет напрямую работать с браузером (доступ к DOM, возможно - отладка). Также имеет доступ к консоли (Terminal), т.е. LLM может выполнять команды операционной системы.

Можно добавить дополнительные MCP-сервера через магазин plug-in-ов.

Почему доступ для разработчиков в сильных моделях стоит дорого: LLM активно взаимодействует с агентами через MCP, в частности, для устранения ошибок сборки и улучшения кода на основании Linter-ов. Такого рода взаимодействие требует огромного количества промптов.

Выгрузка больших проектов, т.е. создание большого контекста, также приводит к активному расходу токенов.

Резюмируя причины дороговизны мощного ИИ:

- требуется более мощное оборудование (размер ОЗУ и SSD) для создания инференса. Мощность LLM определяется сотниями миллиардов параметров
- загрузка контекста в инференс из локальной машины требует большего объёма памяти
- частота выполнения запросов к LLM от агентов значительно выше, чем в классической модели

>_Deep learning_ (глубокое обучение) — это подраздел машинного обучения (machine learning), основанный на использовании многослойных искусственных нейронных сетей для автоматического извлечения сложных закономерностей из данных. Сеть обучается методом обратного распространения ошибки (_backpropagation_).
>
>Отличие от классического машинного обучения состоит в том, что признаки в машинном обучении извлекаются экспертами вручную, а в deep learning - автоматически, нейронной сетью.

## Сводная таблица LLM

На первый взгляд, критичным для сравнения характеристиками являются количество параметров языковой модели и размер окна контекста.

Параметры – это численные веса, которые модель обучает на больших корпусах текста. Каждый параметр — скаляр (обычно 32‑ или 16‑битное число), определяющий силу связи между нейронами в сети. Большее количество параметров обычно позволяет захватить более тонкие закономерности языка, улучшая точность, генерацию и способность решать сложные задачи.

Принято сравнивать классы LLM, исходя из количества параметров:

- 7–9B - часто такие LLM размещаются локально
- 14–32B - часто размещаются локально, но требуют дорогостоящего оборудования
- 70B+ - работают в облачной инфраструктуре на специализированном оборудовании

Размер окна контекста – это максимальное количество токенов, которое модель может принимать в одном запросе (входном фрагменте) и учитывать при генерации ответа. Токен — элемент текста, обычно слово, часть слова или символ; в большинстве моделей один токен ≈ 4 символа английского текста. При получении входа модель строит последовательность токенов и применяет механизм _attention_ к каждому из них. Окно ограничивает длину этой последовательности; если вход превышает лимит, старые токены отбрасываются (обычно с начала).

> **Attention** (внимание) — механизм, позволяющий модели оценивать важность каждого токена входной последовательности при формировании представления другого токена.
>
>В трансформерах каждый токен преобразуется в запрос (query), ключ (key) и значение (value). С помощью скалярного произведения запросов и ключей вычисляются веса («attention scores»), которые затем нормируются (softmax) и применяются к значениям. Полученные взвешенные суммы образуют контекстные представления, учитывающие информацию из всех остальных позиций.
>
>Такой подход даёт модели способность захватывать долгосрочные зависимости и гибко фокусироваться на релевантных частях текста, что является ключевым фактором её эффективности.

> У многих моделей “максимум” не всегда равен “полезному” контексту (качество внимания падает на очень длинных диалогах в зависимости от модели). По этой причине, опытные инженеры используют операции "сжатия контекста", т.е. выделения из него только уникальных особенностей, которые используются при продолжении улучшения генерируемых ответов на следующих итерациях.

Также на качество работы LLM влияют архитектурные улучшения (например, более эффективные _attention_‑механизмы), объём и разнообразие обучающих данных, пост-тюнинг, а также техника дообучения. Всё это может дать значительный прирост качества даже при том же числе параметров.

Один из подходов называется _self-consistency/multiple-sample voting_. Суть подхода состоит в том, что LLM генерирует несколько вариантов ответов (разных) и с помощью специального инструментария (вероятно, ещё одной нейронной сети) оценивает релевантность каждого полученного результата текущему срезу программного кода. Пользователь получает наилучший ответ, а остальные удаляются. Подход работает очень хорошо, но требует на один-два порядка большего количества вычислительных ресурсов.

Для генерирования кода, критически важным является разнообразие обучающих данных. Монетизация множества программных инструментов (библиотек) предполагает платное сопровождение, в которое включается доступ к обсуждениям технических вопросов (форумы), связанных с использованием инструментов. Создатели LLM часто выкупают эти обсуждения и обучают на них свои языковые модели. Пример: форумы по библиотеке разработки пользовательского интерфейса приложений на C++ Qt. Доступ к форумам платный. Известно, что OpenAI выкупил у Reddit права на использование их базы данных обсуждений и обучал (вероятно, обучает) на них ChatGPT. Следствием этого является гораздо лучшая генерация кода для конкретных инструментальных средств конкретными большими языковыми моделями.

Для LLM специализирующихся на задачах программирования ключевые метрики следующие:

- Качество редактирования (diff‑мышление): умение минимально править код без поломок
- Надёжность многошаговых изменений: поддержание инвариантов, типов, тестов
- Работа с репозиторием: способность удерживать структуру проекта (даже при RAG/подгрузке файлов)
- Дебаг по логам: причинно‑следственные гипотезы, аккуратные проверки
- Соблюдение формата: JSON, API‑контракты, схемы, строгие инструкции

Сравнительная таблицы открытых и коммерческих LLM:

| Модель | Параметры (≈) | Размер окна контекста |
|--------|---------------|-----------------------|
| **LLaMA 2 7B** | 7 B | 4 KB (≈ 4096 токенов) |
| **LLaMA 2 13B** | 13 B | 4 KB (≈ 4096 токенов) |
| **Mistral 7B‑Instruct** | 7 B | 8 KB (≈ 8192 токенов) |
| **Mixtral‑8×7B‑Instruct** | 46 B (8 × 7 B) | 16 KB (≈ 16384 токенов) |
| **Gemma 2 9B** | 9 B | 8 KB (≈ 8192 токенов) |
| **Phi‑3 mini‑4K** | 3.8 B | 4 KB (≈ 4096 токенов) |
| **Phi‑3 medium‑128K** | 14 B | 128 KB (≈ 131 072 токенов) |
| **Qwen‑1.5‑7B** | 7 B | 8 KB (≈ 8192 токенов) |
| **Qwen‑1.5‑14B** | 14 B | 8 KB (≈ 8192 токенов) |
| **OpenChat‑3.5‑7B** | 7 B | 4 KB (≈ 4096 токенов) |

*Параметры указаны в миллиардах (B). Размер окна контекста – максимальное количество токенов, которое модель может принимать за один запрос.*

| Модель | Параметры (≈) | Размер окна контекста |
|--------|---------------|-----------------------|
| **GPT‑4‑Turbo** (OpenAI) | 100 B + (точные цифры не раскрыты) | 128 KB (≈ 131 072 токенов) |
| **Claude 3 Opus** (Anthropic) | 200 B + (не раскрыто) | 100 KB (≈ 100 000 токенов) |
| **LLaMA 3 70B** | 70 B | 8 KB (≈ 8192 токенов) |
| **Mistral‑Large** | 123 B | 16 KB (≈ 16384 токенов) |
| **Mixtral‑8×22B** | 176 B (8 × 22 B) | 32 KB (≈ 32768 токенов) |
| **Gemma 2 27B** | 27 B | 16 KB (≈ 16384 токенов) |
| **Phi‑3 medium‑128K** | 14 B | 128 KB (≈ 131 072 токенов) |
| **Qwen‑2‑72B** | 72 B | 32 KB (≈ 32768 токенов) |
| **Gemini 1.5‑Pro** (Google) | 140 B + (не раскрыто) | 100 KB (≈ 100 000 токенов) |
| **DeepSeek‑Coder 33B** | 33 B | 8 KB (≈ 8192 токенов) |

Точная информация по современным LLM обычно не раскрывается. Предположительно, что Gemini 3 Pro, GPT 5.2, Cloude 4.5 кратно мощнее LLM предыдущих поколений. Для сравнения: DeepSeek V3, предположительно, имеет 130 миллиардов параметров.

Обобщая:

- Чем больше параметров, тем больше информации закодировано в LLM
- Размер контекста позволяет LLM работать не с несколькими отдельными фразами запроса, а с кодом проекта, или сложными промптами, описывающими разрабатываемую систему комплексно
- Современные LLM знают актуальные стандарты языков программирования и библиотеки
- Пост-тьюнинг позволяет представлять результаты работы в форматах, применимых для автоматизации
- Надёжность многошаговых изменений позволяет улучшать результат работы LLM итеративно, по этапно

## Каким образом можно получить доступ к LLM

На практике, многие разработчики ПО не оплачивают доступ к LLM, используя бесплатные сервисы. Для сравнения:

[Duck.ai](https://duckduckgo.com/?q=DuckDuckGo+AI+Chat&ia=chat&duckai=1) предоставляет бесплатный доступ к моделям:

- GPT-4o mini
- GPT-5 mini
- GPT-OSS 120B
- Llama 4 Scout
- Claude Haiku 3.5
- Mistral Small 3

[ChadGPT.ru](https://ask.chadgpt.ru/) предоставляет платный доступ к моделям (разных расход токенов для разных моделей и режимов работы):

- GPT 5.2
- Grok 4.1
- Claude 4.5 Sonet
- Gemini 3 Flash
- Gemini 3 Pro
- DeepSeek V3.2

Похожие российские агрегаторы: [Chat AI Bot](https://chataibot.ru/), [Merlin Ai](https://www.getmerlin.in/ru).

Китайские LLM предоставляют доступ к некоторым своим сетям бесплатно, но есть ограничения по надёжности и скорости работы:

- [DeepSeek](https://www.deepseek.com/) V3.2
- [Kimi K2](https://www.kimi.com/)
- [Qwen](https://chat.qwen.ai/) 3
- [Zhipu](https://open.bigmodel.cn/) с сетью GLM-4.7. На январь 2026 года не справилась с ростом подписчиков и оставила только платные тарифные планы (GLM Coding Plan)

Не могу подтвердить фактами, но по косвенным признакам, бесплатный доступ к LLM выполняется на отдельных серверах в порядке общей очереди. Т.е. используемая LLM может быть полноценной, но задачи генерации ставятся в очередь и запросы могут выполняться 5-10 минут, могут не выполняться и вообще в этом варианте нет никаких гарантий.

Российские LLM - [GigaChat](https://giga.chat/) и [Алиса](https://alice.yandex.ru/) предоставляют бесплатный доступ. Однако для ряда задач доступ платный. В частности, подписка на Алиса.ПРО стоит 100 рублей/месяц. Однако российские LLM я не рассматриваю в качестве основных инструментов генерации кода поскольку российским разработчикам недоступны огромные dataset-ы кода, такие как: GitHub, [LeetCode](https://leetcode.com/), Stack overflow, и т.д. Также доступность оборудования для обучения в РФ на два-три порядка хуже, чем у компаний США, или Китая. Доступность оборудования влияет как на количество параметров LLM, так и на периодичность выпуска новых версий LLM. Так же важно заметить, что из-за плохого инвестиционного климата, среда для развития ИИ start-up-ов отсутствует, что мешает появлянию действительно инновационных решений.

>Яндекс также активно развивает направление генерации кода, предоставляя plug-in Code Assistant для VSCode и IntelliJ IDEA. Однако это скорее вспомогательный инструмент для CI-решения [SourceCraft](https://sourcecraft.dev/), предлагающего систему для хранения репозитариев и организации сборки приложений. Тариф "Community" - бесплатный, но не позволяет создавать private-репозитарии. Тариф "Про" - 250 руб/месяц. Разница между планами состоит в хранилище данных (2 ГБ vs 10 ГБ) и количестве минут CI-процессов (800 vs 3000). Расширенные квоты для AI-помощника - 700 руб/месяц.

## В чём состоит промпт-инжениринг

При генерации ответа LLM использует случайный выбор, что обусловлено необходимостью создания "живого" языка. Если бы из возможных вариантов продолжения фразы LLM использовала бы только наилучший, текст был бы механизированным и состоял бы из большого количества повторов. Люди, когда пишут текст, заменяют повторны на синонимы, а LLM выбирает случайных вариант из топа релевантных.

Что бы минимизировать случайность, запросы должны быть очень конкретными, вводя явные ограничения. Чем больше ограничений, тем меньше случайности. Ниже приведён пример запроса с уточнениями:

```
Напиши функцию на Python, которая принимает список чисел и возвращает новый список, содержащий только те элементы, которые являются простыми числами.  

- Используй типизацию (type hints).  
- Добавь docstring с описанием функции и примерами использования.  
- Оптимизируй проверку простоты, используя проверку деления только до √n.  
- Приведи тесты с использованием unittest, покрывающие граничные случаи (пустой список, список без простых чисел, список с отрицательными числами).  
```

Промпт-инженеры часто используют **Markdown** при написании промптов для LLM, потому что он позволяет структурировать запрос — заголовки, списки, кодовые блоки и форматирование — что делает инструкцию более читаемой и помогает модели лучше понять желаемый вывод. В КБ ДОРС мы документируем принятые технические решения, а также описываем протоколы взаимодействия на **Markdown**.

Некоторые промпт-инженеры разрабатывают промпты размером в несколько страниц (до десятка страниц). В определённом смысле, хороший промпт похож на постановку стажёру задачи на разработку в письменном варианте. Отличия состоят в том, что стажёру часто нужно объяснять основные алгоритмы, особенности языка и разницу между хорошими и плохими подходами, а LLM это уже знает. С другой стороны, умный стажёр может понять, что при реализации поставленной задачи "что-то пошло не так" и начнёт на это адекватно реагирования. У LLM "что-то не так" обычно не идёт и генерация просто заканчивается галлюцинацией.

Отношения промпт-инженера и LLM напоминают senior и junior-программистов. Первый ставит задачу и проверяет результат (code review), а второй генерирует код по детальному описанию.

## В чем проблема генерации код ИИ

Рекомендуется для ознакомления статья [Совсем не вайбовый вайбкодинг. Обзор SDD+ фреймворков для разработки с ИИ](https://habr.com/ru/articles/985990/) by comol85. Автор статьи указывает на то, что современные ИИ уже генерируют код лучше, чем средний программист. Проблемы у сгенерированного ИИ кода возникают при развитии проекта:

- Проект выглядит как разрозненные куски кода
- Код ИИ не соответствует правилам безопасности, публичным или корпоративным стандартам.
- При разработке новой фичи ИИ очень часто ломает старую
- Расход контекста и токенов чрезмерно большой
- ИИ пишет новую фичу вместо того чтобы внести модификацию в уже разработканную
- ИИ нужно каждый раз заново "объяснять" всю логику проекта

## Нужно ли использовать английский язык для описания prompt

В общем случае, это не обязательно, т.к. текст промпта сначала токенизируется в вектор целочисленных значений, а затем уже используется для генерации ответа. Текст на русском языке и текст на английском языке, с учётом синонимичности (_embedding_), будет формировать похожие вектора и генерировать схожие ответы.

Однако задача преобразования слова в числовой токен не такая простая как может показаться. Предположим, что мы исследуем оптические процессы и нам нужно знать, что такое "кольцевая мира". Для понимания: мира в оптике, производное от французского слова _mira_, т.е. мишень, т.е. это особенный тип оптического (и магнитного) шаблона, предназначенный для исследования и настройки приборов в оптике. Но если не задать правильный контекст, то при токенизации слово "мира" окажется близким по смыслу с такими словами, как "peace" и "world", а совсем не мишень.

Таким образом, использование английского языка может помочь ИИ корректно выполнить токенизацию запроса и, таким образом, выдать более точный ответ.

С другой стороны, зарубежные модели токенизируют русский язык менее эффективно. Из-за особенностей морфологии и редкости русских слов в обучающих данных фразы разбиваются на большее количество токенов.
