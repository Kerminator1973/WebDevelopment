# Агентская модель в LLM

MCP = _Model Context Protocol_.

Разработчик MCP - Anthropic, в 2024 году.

RAG (Retrieval-Augmented Generation) в LLM — это техника, которая улучшает ответы больших языковых моделей, позволяя им получать и использовать внешние, актуальные данные (из баз знаний, документов, интернета) перед генерацией ответа, а не полагаться только на свои "замороженные" обучающие данные. Вместо того чтобы полагаться только на знания, запомненные в параметрах модели, система динамически подбирает релевантные фрагменты из внешних источников (баз знаний, документов, веб‑страниц) и использует их как контекст для генерации ответа.

Как работает процесс:

- Пользователь вводит вопрос. Специальный encoder преобразует запрос в вектор
- Вектор сравнивается с векторами документов. Возвращаются топ‑k наиболее релевантных фрагментов
- Найденные фрагменты конкатенируются с запросом. При необходимости добавляются подсказки (prompt engineering)
- LLM генерирует текст, опираясь на предоставленный контекст. Поскольку модель «видит» актуальные данные, она может давать более точные и свежие ответы, чем при чистой генерации
- Маленькая модель‑классификатор проверяет, соответствует ли ответ найденному контексту, и при необходимости запрашивает дополнительные фрагменты

Пример подсказки:

```
Вопрос: {query}
Контекст: {retrieved_texts}
Ответь, используя только информацию из контекста.
```

MCP - это как USB-C для агентских систем.

LLM - это клиент агентских систем.

Примеры агентов: PostgreSQL, Google Drive, GitHub.

Чаще всего агенты предоставляют LLM доступ к ресурсам: папкам и файлам, что позволяет LLM сформировать больший контекст, получая доступ к файлам проекта, настройкам, и т.д.

Агенты встраиваются в IDE: [Windsurf](https://windsurf.com/) (см. Cascade), [Cursor](https://cursor.com/) (см. Composer), [ZED](https://zed.dev/) разработанный на Rust, [Google Antigravity](https://antigravity.google/), [Trae IDE](https://www.trae.ai/), Visual Studio Code, Visual Studio 2026, GigaIDE, KodaCode, IntelliJ AI, Amazon CodeWhisperer Studio, GitHub Codespaces. В большинстве случаев, в качестве основы IDE используется Visual Studio Code.

Российский агент встраиваемый в VSCode - [KodaCode](https://kodacode.ru/). Работает с зарубежными LLM без прокси. Поддерживает работу бесплатный режим с китайскими LLM (MiniMax M2.1, Qwen3, Kimi K2 Thinking, GLM 4.6). Требует автризации на GitHub. На начало 2026 года бесплатный, но планируется переход на подписочную модель с поддержкой нескольких бесплатных LLM. В начале 2026 поддержка IDE JetBrains. Разработчики - бывшие сотрудники СБЕРа, разрабатывавшие GigaCode.

>Cursor - в режиме **Plan** задаёт уточняющие вопросы и формирует Markdown-файл с описанием задачи и списком того, чтоб нужно сделать с кодом. Далее Cursor работает уже с этим списком.
>
>В бесплатной версии есть GPT 5.2. Хорошо подходит для проектирования, прототипов и быстрых экспериментов. Позволяет выполнять регистрацию на "10 минутные" почтовые аккаунты.
>
>Примеры 10-минутной (disposable) почты: [TMailor.com](https://tmailor.com/), [Moakt](https://moakt.com/ru), [MailSAC](https://mailsac.com/).
>
>Рекомендуется для прочтения [Осознанный вайб-кодинг](https://habr.com/ru/articles/982452/) by davidaganov. Интересен подход создания первоначального контекста для генерации приложения:
>
>- Появляется идея. Например, сервис для учёта финансов. Я обсуждаю её с ChatGPT: в каком виде лучше реализовать, на каких технологиях, как это можно хостить, что в итоге хочется получить. Прошу задать 10–15 вопросов, ответы на которые помогут при построении плана.
>-После этого прошу построить план с учётом моих ответов и контекста диалога. Получаю MD файл, внимательно его просматриваю, собираю свои вопросы, что-то уточняю и правлю. Когда файл становится понятным и логичным - считаю его готовым к работе.
>-Кладу этот файл в пустую папку проекта, передаю его как контекст в чат и прошу построить архитектуру и установить зависимости. Параллельно вручную накидываю свои конфиги для eslint, prettier, editorconfig и правлю мелочи, которые для меня важны и привычны.

Рекомендуется для прочтения статья [Claude Code научили работать с Chrome. Вот насколько это опасно](https://habr.com/ru/news/978712/) by **python_leader**. Появилась новый вид уязвимостей **Prompt Injection** - документы, которые включаются в контекст LLM, например, электронные письма содержат вставки, вынуждающие LLM формировать команды, наносящие ущерб пользователю системы, например, удалять всю почту пользователя без подтверждения. Браузерные расширения Anthropic развиваются в направлении создания специальной политики информационной безопасности, которая требует подтверждения пользователя для критичных операций выполняемых агентами, таких как: публикации, покупки, предоставление другим пользователям доступа к ресурсам системы.

Agent Mode в Visual Studio Code: `File -> Preference -> Settings`, искать настройку `chat.agent.enabled`.

Agent Mode в Visual Studio Code получает доступ к окну "Output" (результаты компиляции), Linter-у, умеет напрямую работать с браузером (доступ к DOM, возможно - отладка). Также имеет доступ к консоли (Terminal), т.е. LLM может выполнять команды операционной системы.

Можно добавить дополнительные MCP-сервера через магазин plug-in-ов.

Почему доступ для разработчиков в сильных моделях стоит дорого: LLM активно взаимодействует с агентами через MCP, в частности, для устранения ошибок сборки и улучшения кода на основании Linter-ов. Такого рода взаимодействие требует огромного количества промптов.

Выгрузка больших проектов, т.е. создание большого контекста, также приводит к активному расходу токенов.

Резюмируя причины дороговизны мощного ИИ:

- требуется более мощное оборудование (размер ОЗУ и SSD) для создания инференса. Мощность LLM определяется сотниями миллиардов параметров
- загрузка контекста в инференс из локальной машины требует большего объёма памяти
- частота выполнения запросов к LLM от агентов значительно выше, чем в классической модели
