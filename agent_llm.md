# Агентская модель в LLM

MCP = _Model Context Protocol_.

Разработчик MCP - Anthropic, в 2024 году.

RAG (Retrieval-Augmented Generation) в LLM — это техника, которая улучшает ответы больших языковых моделей, позволяя им получать и использовать внешние, актуальные данные (из баз знаний, документов, интернета) перед генерацией ответа, а не полагаться только на свои "замороженные" обучающие данные.

MCP - это как USB-C для агентских систем.

LLM - это клиент агентских систем.

Примеры агентов: PostgreSQL, Google Drive, GitHub.

Чаще всего агенты предоставляют LLM доступ к ресурсам: папкам и файлам, что позволяет LLM сформировать больший контекст, получая доступ к файлам проекта, настройкам, и т.д.

Рекомендуется для прочтения статья [Claude Code научили работать с Chrome. Вот насколько это опасно](https://habr.com/ru/news/978712/) by **python_leader**. Появилась новый вид уязвимостей **Prompt Injection** - документы, которые включаются в контекст LLM, например, электронные письма содержат вставки, вынуждающие LLM формировать команды, наносящие ущерб пользователю системы, например, удалять всю почту пользователя без подтверждения. Браузерные расширения Anthropic развиваются в направлении создания специальной политики информационной безопасности, которая требует подтверждения пользователя для критичных операций выполняемых агентами, таких как: публикации, покупки, предоставление другим пользователям доступа к ресурсам системы.

Agent Mode в Visual Studio Code: `File -> Preference -> Settings`, искать настройку `chat.agent.enabled`.

Agent Mode в Visual Studio Code получает доступ к окну "Output" (результаты компиляции), Linter-у, умеет напрямую работать с браузером (доступ к DOM, возможно - отладка). Также имеет доступ к консоли (Terminal), т.е. LLM может выполнять команды операционной системы.

Можно добавить дополнительные MCP-сервера через магазин plug-in-ов.

Почему доступ для разработчиков в сильных моделях стоит дорого: LLM активно взаимодействует с агентами через MCP, в частности, для устранения ошибок сбокри и улучшения кода на основании Linter-ов. Такого рода взаимодействие требует огромного количества промптов.

Выгрузка больших проектов, т.е. создание большого контекста, также приводит к активному расходу токенов.

Резюмируя причины дороговизны мощного ИИ:

- требуется более мощное оборудование (размер ОЗУ и SSD) для создания инференса. Мощность LLM определяется сотниями миллиардов параметров
- загрузка контекста в инференс из локальной машины требует большего объёма памяти
- частота выполнения запросов к LLM от агентов значительно выше, чем в классической модели
